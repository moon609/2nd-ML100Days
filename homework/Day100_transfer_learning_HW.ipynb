{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n",
    "\n",
    "\n",
    "最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n",
    "\n",
    "另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 20:05:49.663001 50244 deprecation_wrapper.py:119] From E:\\Users\\moon\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "781/781 [==============================] - 529s 678ms/step - loss: 1.8782 - acc: 0.4341 - val_loss: 1.1620 - val_acc: 0.6235\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 525s 672ms/step - loss: 1.3261 - acc: 0.5895 - val_loss: 1.1765 - val_acc: 0.6548\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 525s 673ms/step - loss: 1.1264 - acc: 0.6511 - val_loss: 1.1649 - val_acc: 0.6679\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 524s 671ms/step - loss: 1.0188 - acc: 0.6837 - val_loss: 0.9001 - val_acc: 0.7319\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 524s 671ms/step - loss: 0.9375 - acc: 0.7120 - val_loss: 0.8332 - val_acc: 0.7612\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 524s 671ms/step - loss: 0.8915 - acc: 0.7291 - val_loss: 0.9129 - val_acc: 0.7425\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 524s 671ms/step - loss: 0.8511 - acc: 0.7447 - val_loss: 0.7407 - val_acc: 0.7849\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 524s 671ms/step - loss: 0.8205 - acc: 0.7570 - val_loss: 0.7184 - val_acc: 0.7884\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 526s 673ms/step - loss: 0.7940 - acc: 0.7643 - val_loss: 0.7185 - val_acc: 0.7946\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 524s 670ms/step - loss: 0.7709 - acc: 0.7751 - val_loss: 0.7783 - val_acc: 0.7760\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 523s 670ms/step - loss: 0.7577 - acc: 0.7804 - val_loss: 0.7603 - val_acc: 0.7887\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 523s 670ms/step - loss: 0.7448 - acc: 0.7854 - val_loss: 0.7168 - val_acc: 0.8056\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 523s 670ms/step - loss: 0.7305 - acc: 0.7924 - val_loss: 0.6871 - val_acc: 0.8093\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 523s 670ms/step - loss: 0.7233 - acc: 0.7959 - val_loss: 0.6538 - val_acc: 0.8227\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 523s 670ms/step - loss: 0.7106 - acc: 0.8001 - val_loss: 0.6669 - val_acc: 0.8181\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 541s 693ms/step - loss: 0.7024 - acc: 0.8054 - val_loss: 0.7044 - val_acc: 0.8160\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 9659s 12s/step - loss: 0.6926 - acc: 0.8050 - val_loss: 0.6815 - val_acc: 0.8183\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.6921 - acc: 0.8078 - val_loss: 0.6869 - val_acc: 0.8198\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 510s 653ms/step - loss: 0.6805 - acc: 0.8110 - val_loss: 0.7660 - val_acc: 0.7893\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.6773 - acc: 0.8133 - val_loss: 0.7040 - val_acc: 0.8078\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.6680 - acc: 0.8182 - val_loss: 0.7455 - val_acc: 0.8062\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.6618 - acc: 0.8176 - val_loss: 0.7153 - val_acc: 0.8105\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.6587 - acc: 0.8211 - val_loss: 0.6060 - val_acc: 0.8389\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 510s 653ms/step - loss: 0.6554 - acc: 0.8225 - val_loss: 0.6281 - val_acc: 0.8384\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.6535 - acc: 0.8250 - val_loss: 0.6598 - val_acc: 0.8302\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.6462 - acc: 0.8265 - val_loss: 0.6474 - val_acc: 0.8354\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 524s 670ms/step - loss: 0.6420 - acc: 0.8267 - val_loss: 0.6153 - val_acc: 0.8439\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.6405 - acc: 0.8289 - val_loss: 0.7116 - val_acc: 0.8172\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.6356 - acc: 0.8288 - val_loss: 0.5794 - val_acc: 0.8548\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.6354 - acc: 0.8309 - val_loss: 0.6305 - val_acc: 0.8397\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 511s 655ms/step - loss: 0.6385 - acc: 0.8309 - val_loss: 0.6101 - val_acc: 0.8456\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.6314 - acc: 0.8331 - val_loss: 0.5757 - val_acc: 0.8562\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 528s 676ms/step - loss: 0.6303 - acc: 0.8322 - val_loss: 0.5815 - val_acc: 0.8492\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.6320 - acc: 0.8336 - val_loss: 0.6234 - val_acc: 0.8388\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 20310s 26s/step - loss: 0.6254 - acc: 0.8334 - val_loss: 0.6482 - val_acc: 0.8312\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 521s 667ms/step - loss: 0.6238 - acc: 0.8354 - val_loss: 0.6068 - val_acc: 0.8494\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 520s 666ms/step - loss: 0.6213 - acc: 0.8362 - val_loss: 0.6446 - val_acc: 0.8323\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 520s 666ms/step - loss: 0.6179 - acc: 0.8380 - val_loss: 0.6279 - val_acc: 0.8404\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 520s 666ms/step - loss: 0.6137 - acc: 0.8408 - val_loss: 0.6588 - val_acc: 0.8346\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 520s 666ms/step - loss: 0.6123 - acc: 0.8396 - val_loss: 0.5889 - val_acc: 0.8529\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 521s 668ms/step - loss: 0.6135 - acc: 0.8398 - val_loss: 0.6204 - val_acc: 0.8445\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 519s 664ms/step - loss: 0.6083 - acc: 0.8407 - val_loss: 0.6089 - val_acc: 0.8452\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 519s 664ms/step - loss: 0.6092 - acc: 0.8421 - val_loss: 0.6800 - val_acc: 0.8297\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 518s 664ms/step - loss: 0.6047 - acc: 0.8450 - val_loss: 0.5913 - val_acc: 0.8552\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 520s 666ms/step - loss: 0.6019 - acc: 0.8450 - val_loss: 0.5999 - val_acc: 0.8451\n",
      "Epoch 46/125\n",
      "781/781 [==============================] - 519s 664ms/step - loss: 0.6045 - acc: 0.8431 - val_loss: 0.5939 - val_acc: 0.8512\n",
      "Epoch 47/125\n",
      "781/781 [==============================] - 519s 664ms/step - loss: 0.5999 - acc: 0.8450 - val_loss: 0.5962 - val_acc: 0.8520\n",
      "Epoch 48/125\n",
      "781/781 [==============================] - 521s 666ms/step - loss: 0.6043 - acc: 0.8424 - val_loss: 0.6284 - val_acc: 0.8461\n",
      "Epoch 49/125\n",
      "781/781 [==============================] - 27314s 35s/step - loss: 0.5958 - acc: 0.8451 - val_loss: 0.6154 - val_acc: 0.8472\n",
      "Epoch 50/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5999 - acc: 0.8443 - val_loss: 0.6467 - val_acc: 0.8410\n",
      "Epoch 51/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5956 - acc: 0.8475 - val_loss: 0.6145 - val_acc: 0.8448\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.6002 - acc: 0.8445 - val_loss: 0.6571 - val_acc: 0.8352\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5971 - acc: 0.8464 - val_loss: 0.5773 - val_acc: 0.8593\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5984 - acc: 0.8457 - val_loss: 0.5921 - val_acc: 0.8525\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5917 - acc: 0.8469 - val_loss: 0.5807 - val_acc: 0.8574\n",
      "Epoch 56/125\n",
      "781/781 [==============================] - 510s 653ms/step - loss: 0.5916 - acc: 0.8488 - val_loss: 0.6580 - val_acc: 0.8360\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5928 - acc: 0.8478 - val_loss: 0.6426 - val_acc: 0.8436\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5893 - acc: 0.8480 - val_loss: 0.5954 - val_acc: 0.8560\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5943 - acc: 0.8475 - val_loss: 0.6407 - val_acc: 0.8406\n",
      "Epoch 60/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5875 - acc: 0.8505 - val_loss: 0.6062 - val_acc: 0.8516\n",
      "Epoch 61/125\n",
      "781/781 [==============================] - 508s 651ms/step - loss: 0.5861 - acc: 0.8502 - val_loss: 0.5602 - val_acc: 0.8667\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 508s 651ms/step - loss: 0.5816 - acc: 0.8518 - val_loss: 0.6561 - val_acc: 0.8447\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 512s 655ms/step - loss: 0.5871 - acc: 0.8503 - val_loss: 0.5457 - val_acc: 0.8683\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5813 - acc: 0.8535 - val_loss: 0.6123 - val_acc: 0.8572\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5850 - acc: 0.8507 - val_loss: 0.6379 - val_acc: 0.8452\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 508s 651ms/step - loss: 0.5802 - acc: 0.8536 - val_loss: 0.6234 - val_acc: 0.8467\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5777 - acc: 0.8519 - val_loss: 0.5668 - val_acc: 0.8646\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 508s 651ms/step - loss: 0.5820 - acc: 0.8520 - val_loss: 0.5852 - val_acc: 0.8593\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5846 - acc: 0.8506 - val_loss: 0.5881 - val_acc: 0.8581\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 510s 653ms/step - loss: 0.5858 - acc: 0.8517 - val_loss: 0.6425 - val_acc: 0.8430\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5783 - acc: 0.8537 - val_loss: 0.5943 - val_acc: 0.8559\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5773 - acc: 0.8567 - val_loss: 0.5859 - val_acc: 0.8571\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5752 - acc: 0.8545 - val_loss: 0.5988 - val_acc: 0.8552\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 510s 653ms/step - loss: 0.5782 - acc: 0.8551 - val_loss: 0.6363 - val_acc: 0.8425\n",
      "Epoch 75/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.5725 - acc: 0.8564 - val_loss: 0.5811 - val_acc: 0.8601\n",
      "Epoch 76/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5742 - acc: 0.8554 - val_loss: 0.5899 - val_acc: 0.8586\n",
      "Epoch 77/125\n",
      "781/781 [==============================] - 526s 674ms/step - loss: 0.5276 - acc: 0.8709 - val_loss: 0.5476 - val_acc: 0.8719\n",
      "Epoch 78/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5138 - acc: 0.8737 - val_loss: 0.4980 - val_acc: 0.8849\n",
      "Epoch 79/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.5063 - acc: 0.8760 - val_loss: 0.5230 - val_acc: 0.8744\n",
      "Epoch 80/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4995 - acc: 0.8760 - val_loss: 0.5456 - val_acc: 0.8678\n",
      "Epoch 81/125\n",
      "781/781 [==============================] - 510s 653ms/step - loss: 0.4981 - acc: 0.8768 - val_loss: 0.5180 - val_acc: 0.8729\n",
      "Epoch 82/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4901 - acc: 0.8769 - val_loss: 0.5404 - val_acc: 0.8687\n",
      "Epoch 83/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4884 - acc: 0.8776 - val_loss: 0.5218 - val_acc: 0.8750\n",
      "Epoch 84/125\n",
      "781/781 [==============================] - 511s 654ms/step - loss: 0.4845 - acc: 0.8788 - val_loss: 0.5495 - val_acc: 0.8625\n",
      "Epoch 85/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4807 - acc: 0.8791 - val_loss: 0.5465 - val_acc: 0.8664\n",
      "Epoch 86/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4794 - acc: 0.8789 - val_loss: 0.5147 - val_acc: 0.8718\n",
      "Epoch 87/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4746 - acc: 0.8791 - val_loss: 0.5104 - val_acc: 0.8772\n",
      "Epoch 88/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4734 - acc: 0.8797 - val_loss: 0.4842 - val_acc: 0.8836\n",
      "Epoch 89/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4702 - acc: 0.8815 - val_loss: 0.4949 - val_acc: 0.8825\n",
      "Epoch 90/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4721 - acc: 0.8802 - val_loss: 0.5279 - val_acc: 0.8697\n",
      "Epoch 91/125\n",
      "781/781 [==============================] - 512s 655ms/step - loss: 0.4688 - acc: 0.8814 - val_loss: 0.5214 - val_acc: 0.8749\n",
      "Epoch 92/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4671 - acc: 0.8806 - val_loss: 0.5842 - val_acc: 0.8546\n",
      "Epoch 93/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4657 - acc: 0.8801 - val_loss: 0.5016 - val_acc: 0.8764\n",
      "Epoch 94/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.4699 - acc: 0.8795 - val_loss: 0.5280 - val_acc: 0.8706\n",
      "Epoch 95/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4591 - acc: 0.8837 - val_loss: 0.5009 - val_acc: 0.8768\n",
      "Epoch 96/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4602 - acc: 0.8809 - val_loss: 0.4963 - val_acc: 0.8777\n",
      "Epoch 97/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4646 - acc: 0.8812 - val_loss: 0.5504 - val_acc: 0.8650\n",
      "Epoch 98/125\n",
      "781/781 [==============================] - 510s 653ms/step - loss: 0.4611 - acc: 0.8814 - val_loss: 0.5025 - val_acc: 0.8801\n",
      "Epoch 99/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4605 - acc: 0.8803 - val_loss: 0.4873 - val_acc: 0.8815\n",
      "Epoch 100/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4621 - acc: 0.8805 - val_loss: 0.5089 - val_acc: 0.8754\n",
      "Epoch 101/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.4566 - acc: 0.8807 - val_loss: 0.5467 - val_acc: 0.8681\n",
      "Epoch 102/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4345 - acc: 0.8895 - val_loss: 0.4892 - val_acc: 0.8826\n",
      "Epoch 103/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4248 - acc: 0.8923 - val_loss: 0.4704 - val_acc: 0.8884\n",
      "Epoch 104/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4257 - acc: 0.8922 - val_loss: 0.4660 - val_acc: 0.8868\n",
      "Epoch 105/125\n",
      "781/781 [==============================] - 526s 673ms/step - loss: 0.4208 - acc: 0.8928 - val_loss: 0.4714 - val_acc: 0.8849\n",
      "Epoch 106/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4261 - acc: 0.8909 - val_loss: 0.4730 - val_acc: 0.8881\n",
      "Epoch 107/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4197 - acc: 0.8921 - val_loss: 0.4440 - val_acc: 0.8942\n",
      "Epoch 108/125\n",
      "781/781 [==============================] - 520s 665ms/step - loss: 0.4154 - acc: 0.8938 - val_loss: 0.4522 - val_acc: 0.8900\n",
      "Epoch 109/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.4135 - acc: 0.8951 - val_loss: 0.4493 - val_acc: 0.8895\n",
      "Epoch 110/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.4140 - acc: 0.8936 - val_loss: 0.4560 - val_acc: 0.8877\n",
      "Epoch 111/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4140 - acc: 0.8934 - val_loss: 0.4663 - val_acc: 0.8862\n",
      "Epoch 112/125\n",
      "781/781 [==============================] - 510s 653ms/step - loss: 0.4044 - acc: 0.8960 - val_loss: 0.4686 - val_acc: 0.8826\n",
      "Epoch 113/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.4091 - acc: 0.8935 - val_loss: 0.4739 - val_acc: 0.8818\n",
      "Epoch 114/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.4122 - acc: 0.8939 - val_loss: 0.4577 - val_acc: 0.8877\n",
      "Epoch 115/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.4073 - acc: 0.8959 - val_loss: 0.4631 - val_acc: 0.8840\n",
      "Epoch 116/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4109 - acc: 0.8930 - val_loss: 0.4533 - val_acc: 0.8889\n",
      "Epoch 117/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4052 - acc: 0.8953 - val_loss: 0.4742 - val_acc: 0.8810\n",
      "Epoch 118/125\n",
      "781/781 [==============================] - 509s 652ms/step - loss: 0.4018 - acc: 0.8963 - val_loss: 0.4697 - val_acc: 0.8826\n",
      "Epoch 119/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 516s 661ms/step - loss: 0.4032 - acc: 0.8947 - val_loss: 0.4789 - val_acc: 0.8777\n",
      "Epoch 120/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.4047 - acc: 0.8957 - val_loss: 0.4559 - val_acc: 0.8858\n",
      "Epoch 121/125\n",
      "781/781 [==============================] - 508s 651ms/step - loss: 0.3988 - acc: 0.8964 - val_loss: 0.4553 - val_acc: 0.8869\n",
      "Epoch 122/125\n",
      "781/781 [==============================] - 508s 651ms/step - loss: 0.4012 - acc: 0.8957 - val_loss: 0.4874 - val_acc: 0.8790\n",
      "Epoch 123/125\n",
      "781/781 [==============================] - 508s 650ms/step - loss: 0.3979 - acc: 0.8973 - val_loss: 0.4368 - val_acc: 0.8921\n",
      "Epoch 124/125\n",
      "781/781 [==============================] - 509s 651ms/step - loss: 0.3972 - acc: 0.8961 - val_loss: 0.4440 - val_acc: 0.8892\n",
      "Epoch 125/125\n",
      "781/781 [==============================] - 508s 651ms/step - loss: 0.3919 - acc: 0.8972 - val_loss: 0.4304 - val_acc: 0.8952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fac4da0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training\n",
    "batch_size = 64\n",
    " \n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 30s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test result: 89.520 loss: 0.430\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
